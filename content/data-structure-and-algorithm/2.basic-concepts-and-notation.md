---
title: "Introduction"
metaTitle: "Basic concepts and notations | DevBucket"
metaDescription: ""
---


## Basic concepts and notations


## Complexities
  - **Time complexity:** The amount of time it takes to run an algorithm as the size of the input data grows. It's often denoted by the big O notation (e.g., O(n), O(n^2), O(log n)).
  - **Space complexity:** The amount of memory an algorithm uses as the size of the input data grows. It's often denoted by the big O notation (e.g., O(n), O(n^2), O(log n)).

### Time Complexity

Time complexity is a measure of the amount of time required to run an algorithm as a function of the input size. It is represented by big O notation (e.g: `O(n)`, `O(n^2)`, `O(log n)`), which describes the upper bound of the running time of the algorithm. Time complexity is crucial when designing and analyzing algorithms as it helps to choose the most efficient algorithm to solve a problem and optimize program performance.

It is not a measurement of how much time it takes to execute a particular algorithm but rather the number of times a particular instruction set is executed. The total time taken depends on external factors such as the compiler used and the processor's speed. Time complexity is a type of computational complexity that describes the time required to execute an algorithm and is highly dependent on the size of the processed data. Understanding time complexity is crucial in designing and analyzing algorithms to optimize program performance.

For example, let's consider a simple algorithm that searches for a specific element in an array of n elements. A naive approach to implementing this algorithm is to check each element of the array in order until the target element is found or until the end of the array is reached. The time complexity of this algorithm is O(n) because the worst-case scenario is that the target element is at the end of the array, requiring n comparisons.

Now, let's consider a more efficient algorithm, such as binary search. Binary search is a divide-and-conquer algorithm that works by repeatedly dividing the search interval in half until the target element is found. The time complexity of binary search is O(log n), which is much faster than the naive approach for large values of n.

In summary, time complexity is a crucial factor to consider when designing and analyzing algorithms. By understanding the time complexity of an algorithm, we can choose the most efficient algorithm to solve a problem and optimize program performance.


### Steps to calculate Time Complexity

There are different methods to calculate time complexity, but the most common one is to count the number of operations performed by the algorithm as a function of the input size. Here are the general steps to follow:

- Identify the basic operations performed by the algorithm, such as assignments, comparisons, and loops.
- Determine how many times each operation is executed based on the size of the input.
- Express the total number of operations as a function of the input size, using big O notation to represent the upper bound.
- Simplify the function by dropping the lower order terms and constant factors.
- Interpret the resulting function as the time complexity of the algorithm.

For example, let's consider the following algorithm that calculates the sum of an array of n numbers:

```Python
sum = 0
for i = 0 to n-1
   sum = sum + array[i]
return sum
```

To calculate the time complexity of this algorithm, we can follow these steps:

- The basic operations are the assignment of `0` to `sum`, the addition of `array[i]` to sum, and the comparison of `i` to `n-1`.
- The assignment of `0` to `sum` is performed only once, so it contributes `O(1)` to the time complexity. 
- The addition of `array[i]` to sum is performed `n` times, so it contributes `O(n)` to the time complexity. 
- The comparison of `i` to `n-1` is performed `n` times, so it contributes `O(n)` to the time complexity.
- The total number of operations is  `1 + n + n`, which is `2n + 1`. Therefore, the time complexity of the algorithm is `O(n)`.
- Since we drop the constant factor and the lower order term, the time complexity can be simplified as `O(n)`.
- Interpretation: The time complexity of this algorithm is linear, meaning that it grows in proportion to the input size. This is a relatively efficient algorithm, as its running time is proportional to the size of the input.

Keep in mind that the calculation of time complexity is an approximation and can vary depending on the specific implementation, hardware, and other external factors. However, it is still a useful tool to compare different algorithms and choose the most efficient one for a specific problem.

## Space complexity

Space complexity is a measure of the amount of memory required by an algorithm as a function of input size. In other words, it is the amount of memory used by an algorithm to solve a problem as the size of the input increases. Space complexity is usually represented by big O notation (e.g: `O(n)`, `O(n^2)`, `O(log n)`), which describes the upper bound of the memory usage of the algorithm.

Similar to time complexity, space complexity is an important factor to consider when designing and analyzing algorithms. By understanding the space complexity of an algorithm, we can choose the most efficient algorithm to solve a problem and optimize the memory usage of our programs.

For example, let's consider an algorithm that creates an array of n elements and then iterates over the array to perform some computation. The space complexity of this algorithm is O(n), as it requires storing an array of n elements in memory. On the other hand, an algorithm that operates on the input data without creating an array may have a lower space complexity.

It's important to note that space complexity and time complexity are not always directly correlated. An algorithm with a low space complexity may have a high time complexity and vice versa. Therefore, it's important to consider both time and space complexity when analyzing and optimizing algorithms.


### Steps to calculate Space Complexity

To calculate the space complexity of an algorithm, we need to consider the additional space used by the algorithm in addition to the input size. This additional space can come from variables, data structures, and other factors that are used by the algorithm during its execution.

Here are the steps to calculate the space complexity of an algorithm:

- Identify the additional space used by the algorithm in addition to the input size. This can include variables, data structures, and any other space used during the execution of the algorithm.
- Express the additional space used by the algorithm as a function of the input size.
- Simplify the function to identify the highest-order term. This term will represent the space complexity of the algorithm.
- Represent the space complexity using big O notation.

For example, let's consider the following algorithm that sums the elements in an array of size n:

```Python
function sumArray(array):
    sum = 0
    for i in range(n):
        sum += array[i]
    return sum
```
To calculate the space complexity of this algorithm, we need to consider the additional space used by the algorithm, which is the space used by the sum variable. 
- Since this variable is a single integer, its space requirement is constant and does not depend on the size of the input array. 
- Therefore, the space complexity of this algorithm is O(1).

In summary, calculating the space complexity of an algorithm involves identifying the additional space used by the algorithm and expressing it as a function of the input size, then simplifying the function to identify the highest-order term and representing the space complexity using big O notation.

## Asymptotic analysis using Big O notation
  - Big-O Notation (Ο)
  - Omega Notation (Ω)
  - Theta Notation (θ)

## Recurrence relations

## Greedy algorithms


## Resources
- https://www.geeksforgeeks.org/time-complexities-of-different-data-structures/
- 