---
title: "Complexities and Notations"
metaTitle: "Complexities and Notations | DevBucket"
metaDescription: ""
---

## Algorithm Complexity
Algorithm complexity refers to the measure of how much time and/or space an algorithm requires to solve a problem as the size of the input grows. It is a fundamental concept in computer science and plays a crucial role in determining the efficiency and scalability of algorithms. By analyzing the complexity of an algorithm, developers can make informed decisions about selecting the most appropriate algorithm for a given problem and optimize the performance of their code.

**Common algorithm complexities include:**

Algorithm complexity refers to the amount of resources an algorithm requires to solve a problem. These resources are usually measured in terms of time and space, which indicate how the time and space requirements of the algorithm grow with the input size. Here are some common types of algorithm complexity:
- **Time complexity:** 
  - The amount of time an algorithm takes to complete as the input size increases. This is often denoted by big O notation, such as O(n), O(n^2), or O(log n). 
  - Time complexity can be expressed using different terms like constant, logarithmic, linear, quadratic, cubic, or exponential depending on how they grow with the input size.

- **Space complexity:** 
  - The amount of memory an algorithm requires as the input size increases. 
  - Like time complexity, space complexity is also expressed using big O notation, such as O(n), O(n^2), or O(log n).


## Time Complexity

Time complexity is a measure of the amount of time required to run an algorithm as a function of the input size. It is represented by big O notation (e.g: `O(n)`, `O(n^2)`, `O(log n)`), which describes the **upper bound** of the running time of the algorithm. Time complexity is crucial when designing and analyzing algorithms as it helps to choose the most efficient algorithm to solve a problem and optimize program performance.

It is not a measurement of how much time it takes to execute a particular algorithm but rather the number of times a particular instruction set is executed. The total time taken depends on external factors such as the compiler used and the processor's speed. Time complexity is a type of computational complexity that describes the time required to execute an algorithm and is highly dependent on the size of the processed data. Understanding time complexity is crucial in designing and analyzing algorithms to optimize program performance.

For example, let's consider a simple algorithm that searches for a specific element in an array of `n` elements. A naive approach to implementing this algorithm is to check each element of the array in order until the target element is found or until the end of the array is reached. The time complexity of this algorithm is `O(n)` because the worst-case scenario is that the target element is at the end of the array, requiring `n` comparisons.

Now, let's consider a more efficient algorithm, such as binary search. Binary search is a divide-and-conquer algorithm that works by repeatedly dividing the search interval in half until the target element is found. The time complexity of binary search is `O(log n)`, which is much faster than the naive approach for large values of `n`.

In summary, time complexity is a crucial factor to consider when designing and analyzing algorithms. By understanding the time complexity of an algorithm, we can choose the most efficient algorithm to solve a problem and optimize program performance.

### Types of Time Complexity
The time taken for an algorithm consists of two main components:

- **Compilation time:** 
  - This is the time taken by the compiler to convert the program code into machine code that can be executed by the computer. 
  - While compiling it checks for the syntax and semantic errors in the program and links it with the standard libraries.
  - The compilation time of a program depends on various factors, such as the size and complexity of the code, the compiler used, and the system specifications. 
  - However, it is not typically considered in the analysis of algorithmic time complexity, which focuses on the computational resources required to execute an algorithm as a function of the input size.

- **Run time:** 
  - This is the time taken by the program to execute on the computer. 
  - Note that run time is calculated for executable statement and not for declaration statements
  - It is typically expressed in terms of the "Big O" notation, which describes the upper bound of the algorithm's running time as a function of the input size.
  - The run time depends on the algorithm used, the input size, the efficiency of the implementation, and the hardware resources available.
  - Runtime complexity is affected by the number of basic operations performed by an algorithm. Basic operations include arithmetic operations, assignments, and comparisons. The number of basic operations performed by an algorithm can vary depending on the input size and the specific algorithm used to solve the problem.
  - For example, consider the problem of searching for an element in an array. A linear search algorithm, which checks each element in the array one by one until the target element is found, has a runtime complexity of `O(n)`, where `n` is the size of the array. On the other hand, a binary search algorithm, which uses a divide-and-conquer approach to search for the element, has a runtime complexity of `O(log n)`.
  - In general, algorithms with lower runtime complexities are more efficient and desirable for solving problems. 
  - However, it's important to note that the actual runtime of an algorithm can vary depending on the specific input and the hardware on which it is executed.


When discussing time complexity, we typically focus on the run time component, as it is the aspect that is most relevant for analyzing and comparing algorithms.


### Steps to calculate Time Complexity

There are different methods to calculate time complexity, but the most common one is to count the number of operations performed by the algorithm as a function of the input size. Here are the general steps to follow:

- Identify the basic operations performed by the algorithm, such as assignments, comparisons, and loops.
- Determine how many times each operation is executed based on the size of the input.
- Express the total number of operations as a function of the input size, using big O notation to represent the upper bound.
- Simplify the function by dropping the lower order terms and constant factors.
- Interpret the resulting function as the time complexity of the algorithm.

For example, let's consider the following algorithm that calculates the sum of an array of n numbers:

```Python
sum = 0
for i = 0 to n-1
   sum = sum + array[i]
return sum
```

To calculate the time complexity of this algorithm, we can follow these steps:

- The basic operations are the assignment of `0` to `sum`, the addition of `array[i]` to sum, and the comparison of `i` to `n-1`.
- The assignment of `0` to `sum` is performed only once, so it contributes `O(1)` to the time complexity. 
- The addition of `array[i]` to sum is performed `n` times, so it contributes `O(n)` to the time complexity. 
- The comparison of `i` to `n-1` is performed `n` times, so it contributes `O(n)` to the time complexity.
- The total number of operations is  `1 + n + n`, which is `2n + 1`. Therefore, the time complexity of the algorithm is `O(n)`.
- Since we drop the constant factor and the lower order term, the time complexity can be simplified as `O(n)`.
- Interpretation: The time complexity of this algorithm is linear, meaning that it grows in proportion to the input size. This is a relatively efficient algorithm, as its running time is proportional to the size of the input.

Keep in mind that the calculation of time complexity is an approximation and can vary depending on the specific implementation, hardware, and other external factors. However, it is still a useful tool to compare different algorithms and choose the most efficient one for a specific problem.

## Space Complexity

Space complexity is a measure of the amount of memory required by an algorithm as a function of input size. In other words, it is the amount of memory used by an algorithm to solve a problem as the size of the input increases. Space complexity is usually represented by big O notation (e.g: `O(n)`, `O(n^2)`, `O(log n)`), which describes the upper bound of the memory usage of the algorithm.

Similar to time complexity, space complexity is an important factor to consider when designing and analyzing algorithms. By understanding the space complexity of an algorithm, we can choose the most efficient algorithm to solve a problem and optimize the memory usage of our programs.

For example, let's consider an algorithm that creates an array of n elements and then iterates over the array to perform some computation. The space complexity of this algorithm is O(n), as it requires storing an array of n elements in memory. On the other hand, an algorithm that operates on the input data without creating an array may have a lower space complexity.

It's important to note that space complexity and time complexity are not always directly correlated. An algorithm with a low space complexity may have a high time complexity and vice versa. Therefore, it's important to consider both time and space complexity when analyzing and optimizing algorithms.

### Steps to calculate Space Complexity

To calculate the space complexity of an algorithm, we need to consider the additional space used by the algorithm in addition to the input size. This additional space can come from variables, data structures, and other factors that are used by the algorithm during its execution.

Here are the steps to calculate the space complexity of an algorithm:

- Identify the additional space used by the algorithm in addition to the input size. This can include variables, data structures, and any other space used during the execution of the algorithm.
- Express the additional space used by the algorithm as a function of the input size.
- Simplify the function to identify the highest-order term. This term will represent the space complexity of the algorithm.
- Represent the space complexity using big O notation.

For example, let's consider the following algorithm that sums the elements in an array of size n:

```Python
function sumArray(array):
    sum = 0
    for i in range(n):
        sum += array[i]
    return sum
```
To calculate the space complexity of this algorithm, we need to consider the additional space used by the algorithm, which is the space used by the sum variable. 
- Since this variable is a single integer, its space requirement is constant and does not depend on the size of the input array. 
- Therefore, the space complexity of this algorithm is O(1).

In summary, calculating the space complexity of an algorithm involves identifying the additional space used by the algorithm and expressing it as a function of the input size, then simplifying the function to identify the highest-order term and representing the space complexity using big O notation.

### Time vs space complexity

 | Factor           | Time Complexity                                                                                              | Space Complexity                                                                                                        |
 | ---------------- | ------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------- |
 | Definition       | Measure of amount of time taken by an algorithm to run as a function of input size.                          | Measure of amount of memory taken by an algorithm to run as a function of input size.                                   |
 | Representation   | Big O notation                                                                                               | Big O notation                                                                                                          |
 | Depends on       | Operation, Comparision, Loop Stuff, Pointer refences, Function calls to outside                              | Variables, Data Structures, Allocations, Function call                                                                  |
 | What it measures | Number of operations or steps required to complete an algorithm as input size increases.                     | Amount of memory required to store all the variables, data structures, and intermediate values as input size increases. |
 | Importance       | Determines the efficiency of an algorithm and helps to choose the most optimal algorithm to solve a problem. | Determines how much memory an algorithm needs to operate and whether the algorithm can run on a given system.           |
 | Examples         | O(n), O(log n), O(n^2), O(1), etc.                                                                           | O(n), O(log n), O(n^2), O(1), etc.                                                                                      |
 | Calculation      | Based on the number of operations in the algorithm                                                           | Based on the amount of memory required to store variables, data structures, and intermediate values in the algorithm.   |
 | Trade-offs       | Increasing time complexity may lead to decreased space complexity, and vice versa.                           | Increasing space complexity may lead to decreased time complexity, and vice versa.                                      |



## Complexity Classes

Common complexity classes are a set of categories used to classify algorithms based on their time and space complexity. 

Complexity classes, ordered from **fastest to slowest** are:

| Complexity       | Notation   |
| ---------------- | ---------- |
| Constant time    | (O(1))     |
| Logarithmic time | (O(log n)) |
| Linear time      | (O(n))     |
| Log-linear       | O(nlog(n)) |
| Quadratic time   | (O(n^2))   |
| Polynomial time  | (O(n^c))   |
| Cubic Complexity | O(n^3)     |
| Exponential time | (O(c^n))   |
| Factorial time   | (O(n!))    |

- **Constant time (O(1)):** 
  - Constant time `(O(1))` refers to algorithms that require a fixed amount of time to complete, regardless of the size of the input data. 
  - This time complexity is achieved when an algorithm undergoes a constant number of steps, such as 1, 5, or 10, to solve a given problem. 
  - An example of an algorithm with constant time complexity is accessing an element of an array by index. 
  - This means that the count of operations remains the same, independent of the size of the input data. 
  - Similarly, an algorithm has a constant space complexity if it uses the same amount of memory space regardless of the input size. 
  - For instance, a function that returns a single value and does not use any additional data structures has a constant space complexity.

- **Logarithmic time (O(log n)):**  
  - Logarithmic time `(O(log n))` is a time complexity category for algorithms that take an increasing but logarithmic amount of time to complete as the input size grows. 
  - This time complexity indicates that the time required to complete the algorithm increases logarithmically with the input size. 
  - Binary search is an example of an algorithm that exhibits a logarithmic time complexity. Such algorithms typically require `log(N)` steps to perform operations on `N` elements, and the logarithmic base is often taken as 2. 
  - For instance, for `N = 1,000,000`, an algorithm with `O(log(N))` time complexity would require about `20` steps, with constant precision. 
  - The logarithmic base is often omitted since it does not necessarily affect the order of operation count. 
  - Additionally, an algorithm has logarithmic space complexity if the amount of memory required grows logarithmically with the input size. 
  - For instance, a binary search algorithm that divides the input in half at each step has a logarithmic space complexity.
  - A mathematical concept that's widely used in Computer Science and that's defined by the following equation:

    log<sub>b</sub>(x) = y if and only if b<sup>y</sup> = x

  - In the context of coding interviews, the logarithm is used to describe the complexity analysis of algorithms, and its usage always implies a logarithm of base `2`. In other words, the logarithm used in the context of coding interviews is defined by the following equation:

    log(n) = y if and only if 2<sup>y</sup> = n

  - In plain English, if an algorithm has a logarithmic time complexity (O(log(n)), where `n` is the size of the input), then whenever the algorithm's input doubles in size (i.e., whenever `n` doubles), the number of operations needed to complete the algorithm only increases by one unit. Conversely, an algorithm with a linear time complexity would see its number of operations double if its input size doubled.

  - ∏As an example, a linear-time-complexity algorithm with an input of size 1,000 might take roughly 1,000 operations to complete, whereas a logarithmic-time-complexity algorithm with the same input would take roughly 10 operations to complete, since 2<sup>10</sup> ~= 1,000.

- **Linear time (O(n)):** 
  - Linear time `(O(n))` is a time complexity category for algorithms that take an amount of time proportional to the input size to complete. 
  - This time complexity indicates that the time required to complete the algorithm increases linearly with the input size. 
  - An example of an algorithm with a linear time complexity is iterating through an array. In linear time complexity, the number of steps required to implement an operation on `N` elements is proportional to the total number of elements. 
  - For instance, if there are `500` elements, it will take approximately `500` steps. The number of steps for `N` elements can be` N/2` or `3N`, among others. 
  - Linear time complexity also imposes a runtime of `O(nlog(n))`, undergoing the execution of the order `N*log(N)` on `N` elements to solve a given problem. 
  - For instance, for a given `1000` elements, the linear complexity would execute `10,000` steps to solve a problem. 
  - An algorithm has a linear space complexity if the amount of memory required increases linearly with the input size. Storing `n` elements in an array has a linear space complexity.

- **Log-linear: O(nlog(n)):**
  - Log-linear time complexity, denoted as `O(nlog(n))`, is a category of algorithms that take longer to complete as the input size increases, but not as long as quadratic or exponential time algorithms. 
  - It is a combination of linear and logarithmic time complexities, where the time required to complete the algorithm increases proportionally to the input size and the logarithm of the input size. 
  - Examples of algorithms with log-linear time complexity include merge sort and heap sort. 
  - In merge sort, the input array is divided into two halves recursively until they are of size `1`, and then they are merged back together. 
  - The time complexity of merge sort is `O(nlog(n))`, as each half of the array is sorted recursively with `log(n)` operations, and merging the two halves takes `n` operations.
  - Similarly, heap sort builds a binary heap of the input array, which takes `O(n)` time, and then repeatedly extracts the maximum element from the heap, which takes `log(n)` operations, until the array is sorted. 
  - The overall time complexity of heap sort is also `O(nlog(n))`. 
  - Log-linear time complexity is commonly used in sorting algorithms, and its performance is considered efficient for large input sizes. 
  - However, it is still slower than constant, logarithmic, or linear time complexity algorithms.

- **Quadratic time (O(n^2)):** 
  - Algorithms that take time proportional to the square of the input size to run. 
  - An algorithm has a quadratic time complexity if the time required to complete the algorithm increases quadratically with the input size. 
  - For example, nested loops iterating over an array have a quadratic time complexity. 
  - A quadratic time complexity, denoted by `O(n^2)`, implies that an algorithm will perform the order of `n^2` operations on n input elements to solve a given problem. 
  - For instance, for an input size of `N=100`, it would require `10,000` steps. Quadratic complexity arises whenever the order of operation has a quadratic relationship with the input size. 
  - For example, for `N` input elements, the number of steps may be in the order of `3N^2/2`. 
  - An algorithm has a quadratic space complexity if the amount of memory space required increases quadratically with the input size. 
  - For example, storing a two-dimensional matrix of size `n x n` has a quadratic space complexity.

- **Polynomial time (O(n^c)):**
  - Polynomial time algorithms are algorithms whose running time increases as a polynomial function of the input size, where the degree of the polynomial is a constant, denoted by `c`.
  - This means that for a problem of size `n`, the number of operations required to solve it is proportional to n raised to the power of `c`, where `c` is a constant. 
  - For example, an algorithm that takes `O(n^3)` time to run is a polynomial time algorithm, where the running time increases as a cubic function of the input size. 
  - Polynomial time algorithms are generally considered efficient because their running time grows at a relatively slow rate as the input size increases. 
  - However, as the degree of the polynomial increases, the running time can still become impractical for large input sizes. 
  - Examples of polynomial time algorithms include various sorting algorithms such as merge sort and quicksort, as well as algorithms for polynomial interpolation, polynomial regression, and matrix multiplication

- **Cubic Complexity (O(n^3)):**
  - Cubic complexity is a type of time complexity in computer science that refers to algorithms that take time proportional to the cube of the input size to run. 
  - It has a complexity of `O(n^3)`, where `n` is the input size. 
  - This means that for `N` input data size, it will execute the order of `N^3` steps on `N` elements to solve a given problem. 
  - For example, if there are `100` elements, it will execute `1,000,000` steps. 
  - An example of an algorithm with cubic complexity is a nested loop iterating over a three-dimensional array. 
  - Cubic complexity is a relatively slow time complexity and can be problematic for large inputs.

- **Exponential time (O(c^n)):** 
  - These are algorithms that take an amount of time proportional to an exponential function of the input size to run, where `c` is a constant. 
  - An algorithm has an exponential time complexity if the time it takes to complete the algorithm increases exponentially with the input size. 
  - For example, the recursive Fibonacci sequence algorithm has an exponential time complexity. The exponential time complexity is denoted by `O(2^n)`, `O(n!)`, `O(n^k)`, and so on. 
  - It means that the number of operations required to solve a problem is exponentially dependent on the size of the input data. 
  - For example, if the input size is `N=10`, then the exponential function `2^N` will result in `1024`. Similarly, if `N=20`, it will result in `1048576`, and if `N=100`, it will result in a number having `30` digits. 
  - The factorial function `N!` grows even faster; for example, `N=5` will result in `120`, and `N=10` will result in `3,628,800`. 
  - The space complexity of an algorithm can also be measured using similar notations like `O(1)`, `O(n)`, `O(n^2)`, etc. 
  - An algorithm has an exponential space complexity if the amount of memory space required increases exponentially with the input size. 
  - For example, a recursive algorithm that generates all subsets of a set has an exponential space complexity.

- **Factorial time (O(n!)):** 
  - Factorial time complexity refers to algorithms that take time proportional to the factorial of the input size to run. 
  - An algorithm has a factorial time complexity if the time required to complete the algorithm increases factorially with the input size. 
  - This means that as the input size increases, the time required to solve the problem grows at an incredibly rapid pace. 
  - It is the slowest time complexity and is usually avoided in practice. 
  - Since the constants do not hold a significant effect on the order of count of operation, so it is better to ignore them. 
  - Thus, to consider an algorithm to be linear and equally efficient, it must undergo `N`, `N/2` or `3*N` count of operation, respectively, on the same number of elements to solve a particular problem.

## Asymptotic Analysis

Asymptotic analysis is the study of the performance of an algorithm in the limit as the input size goes to infinity. 

**The three main types of asymptotic analysis are:**

- **Worst-case complexity:**
  - The worst-case complexity is the maximum amount of time an algorithm can take to complete for any possible input of size `n`. 
  - It is the upper bound on the running time of the algorithm. 
  - In other words, it describes the scenario in which the algorithm performs the most amount of work.
  - It is an important measure of an algorithm's efficiency as it provides a guarantee that the algorithm will never take more time or space than the worst-case complexity for any input size.
  - An example of worst-case complexity can be demonstrated through a linear search algorithm. In the worst-case scenario, the target element is not present in the list, and we have to compare it with all the `n` elements of the list before determining that it is not present. In this case, the time complexity of the linear search algorithm would be `O(n)`, as the algorithm has to examine every element in the list.
  - For example, let's say we have a list of `10,000` elements and we are searching for an element that is not present in the list. In this worst-case scenario, the linear search algorithm will have to compare the target element with all `10,000` elements before concluding that it is not present. This is the worst-case complexity of the linear search algorithm.

- **Average-case complexity:**
  - The average-case complexity is the expected amount of time an algorithm takes to complete for all possible inputs of size n, assuming a probability distribution for the input. 
  - It is a more realistic measure of an algorithm's performance than the worst-case complexity, as it takes into account the likelihood of different inputs.
  - To calculate the average-case complexity of an algorithm, you typically need to make assumptions about the probability distribution of inputs. For example, if you're analyzing an algorithm that sorts an array, you might assume that all permutations of the array are equally likely to occur as input. You would then calculate the average number of comparisons or swaps required by the algorithm across all possible inputs, weighted by their probabilities.
  - An example of average-case complexity can be illustrated with quicksort algorithm. Quicksort has an average-case time complexity of `O(n*log n)`, which means that the time required to sort an array of size `n` is proportional to `n` times the logarithm of `n`.
  - In the average case, the quicksort algorithm performs reasonably well, as it partitions the array into two sub-arrays and recursively sorts them. However, if the input array is already sorted or contains many duplicate elements, the quicksort algorithm may perform poorly, resulting in a worst-case time complexity of `O(n^2)`.
  - Therefore, in the average case, quicksort's time complexity is faster than its worst-case complexity but slower than its best-case complexi

- **Best-case complexity:**
  - The best-case complexity is the minimum amount of time an algorithm can take to complete for any possible input of size `n`. 
  - It is the lower bound on the running time of the algorithm.
  - It is the scenario where the input is already in the desired state, and the algorithm requires the least possible amount of time or space to process it.
  - However, the best-case complexity is not always a good indicator of the algorithm's overall performance since the best-case scenario may occur rarely or not at all in real-world situations. Therefore, the focus is usually on the average-case or worst-case complexity to analyze the algorithm's performance.
  - An example of the best-case complexity is when an algorithm has to perform only one operation to solve the problem. For instance, consider an algorithm that searches for a specific element in an array, and the target element is the first element in the array. In this case, the algorithm will only need to perform one operation, which is to check if the first element is the target element. Therefore, the best-case time complexity of this algorithm is `O(1)`.
  - However, the worst-case time complexity of this algorithm is `O(n)`, where `n` is the number of elements in the array, if the target element is not present in the array.


In summary, worst-case complexity is a measure of the algorithm's upper bound, while average-case complexity is a measure of its expected performance, and best-case complexity is a measure of its lower bound.


| Complexity Type         | Definition                                                                       | Example                                                                                                                                                                       | Measurement of                                                    |
| ----------------------- | -------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| Worst-case complexity   | The maximum time or space required by an algorithm for any input of size n       | Searching for an element that is not in a sorted list using linear search; the worst case is when the element is not in the list and we have to search through all n elements | It is a measure of the upper bound of an algorithm's performance. |
| Average-case complexity | The expected time or space required by an algorithm for a random input of size n | Quicksort, where the average case occurs when the pivot divides the list into two roughly equal-sized sublists                                                                | It is a measure of the upper bound of an algorithm's performance. |
| Best-case complexity    | The minimum time or space required by an algorithm for any input of size n       | Searching for the first element in an unsorted list; the best case is when the element is the first element in the list, requiring only one comparison                        | It is a measure of the lower bound of an algorithm's performance. |


## Asymptotic Notation

Asymptotic analysis is a technique used to analyze the performance of an algorithm as the input size grows towards infinity. It is a mathematical approach that helps in measuring the time and space complexity of an algorithm.

The basic idea behind asymptotic analysis is to represent the time or space taken by an algorithm in terms of its input size. It provides an upper bound on the running time of the algorithm for a given input size, which is expressed using big O notation.

Asymptotic analysis considers the order of growth of the algorithm as the input size increases. The order of growth refers to the rate at which the running time or space required by the algorithm grows with the increase in the input size. The most common orders of growth are constant time (O(1)), logarithmic time (O(log n)), linear time (O(n)), quadratic time (O(n^2)), and exponential time (O(2^n)).

Asymptotic analysis allows us to compare the performance of different algorithms by analyzing their time and space complexities. For example, an algorithm with a time complexity of O(n) will be more efficient than an algorithm with a time complexity of O(n^2) for large input sizes.

Overall, asymptotic analysis is an important tool in algorithm analysis and helps in designing efficient algorithms.

**Types of asymptotic Notation:**

- **Big-O Notation (Ο):** 
  - Big-O notation, also known as order notation, is a type of asymptotic notation commonly used in computer science to describe the upper bound of the time complexity of an algorithm. It gives an idea of the worst-case scenario for the time or space complexity of an algorithm as the input size increases.
  - For instance, if an algorithm has a time complexity of `O(n)`, it means that the running time of the algorithm will increase linearly with the input size. If the input size doubles, the running time will also double. The notation `O(n)` defines the upper bound of the growth rate of an algorithm's running time with respect to input size `n`, meaning that the actual running time will never exceed `O(n)` as n grows larger.
  - Big-O notation is a useful tool to compare the performance of different algorithms and to describe how well an algorithm scales as the input size increases. It is read as "order of n" or "big O of n," where n represents the input size.


- **Omega Notation (Ω):** 
  - Omega notation (Ω) is a mathematical notation used in computer science to describe the lower bound of the running time of an algorithm. It represents the best-case scenario or the minimum amount of time required to solve a problem. It is the opposite of the Big-O notation (O), which represents the upper bound or worst-case scenario of the algorithm's running time.
  - In simple terms, if an algorithm has a lower bound of Ω(n), it means that the best-case scenario for the running time of the algorithm will be at least n. For example, if we have an algorithm that searches for an element in a sorted list, the best-case scenario would be that the element is found at the first position of the list, and in this case, the algorithm's running time would be constant, i.e., Ω(1).
  - Omega notation is useful in determining the best-case scenario of an algorithm, as it helps to find the lower bound of the running time. It is often used in conjunction with Big-O notation to provide both the upper and lower bounds of an algorithm's running time, giving a more complete picture of its performance.


- **Theta Notation (θ):** 
  - Theta notation (θ) is a mathematical notation used in computer science to describe the tight bound of an algorithm's time complexity. It represents both the upper and lower bounds of the growth rate of a function and gives an idea of the exact tightest bound for the time or space complexity of an algorithm.
  - In other words, if a function f(n) is said to be θ(g(n)), it means that f(n) grows at the same rate as g(n) up to a constant factor. This notation is useful when we want to describe the exact behavior of an algorithm's time complexity, as it provides an upper and lower bound.
  - For instance, if the time complexity of an algorithm is θ(n), it means that the algorithm's best-case and worst-case scenarios will take at least and at most n time, respectively. This notation provides a more complete and accurate picture of an algorithm's time complexity when used alongside Big-O and Omega notation.



| Notation  | Definition                                                                   | Example                                                                                                                                                   |
| --------- | ---------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Big-O (O) | Represents the upper bound of the growth rate of a function.                 | If the time complexity of an algorithm is O(n), it means that the worst-case scenario will take at most n time.                                           |
| Omega (Ω) | Represents the lower bound of the growth rate of a function.                 | If the time complexity of an algorithm is Ω(n), it means that the best-case scenario will take at least n time.                                           |
| Theta (θ) | Represents both the upper and lower bounds of the growth rate of a function. | If the time complexity of an algorithm is θ(n), it means that the best-case and worst-case scenarios will take at most and at least n time, respectively. |


## Standard Analysis technique

The process of standard analysis technique involves the analysis of an algorithm's time and space complexity by breaking it down into individual operations and determining the number of times each operation is executed. The following methods are commonly used in standard analysis technique:

- **Constant time statements:** These statements have a fixed execution time, such as arithmetic operations, variable assignments, and accessing elements of an array, regardless of the input size. The time complexity of such statements is O(1).
- **Analyzing loops:** The time complexity of a loop is based on the number of iterations it performs, which is usually related to the input size. For example, a for loop that iterates n times has a time complexity of O(n).
- **Analyzing nested loops:** When loops are nested, the time complexity can become more complex, often resulting in O(n^2) or higher time complexity in some cases.
- **Analyzing a sequence of statements:** The time complexities of multiple statements executed in sequence are added together. For instance, executing statement A with a time complexity of O(n) followed by statement B with O(log n) results in a time complexity of O(n + log n), which simplifies to O(n).
- **Analyzing conditional statements:** The time complexity of a conditional statement such as an if statement depends on the complexity of the code inside the statement, as well as the probability of the condition being true. The time complexity will be the same as that of the code inside the statement if the probability is close to 1, and negligible if it is close to 0.
- **Counting the basic operations:** This involves counting the number of basic operations performed by the algorithm, such as arithmetic operations, comparisons, and assignments.
- **Using mathematical formulas:** In some cases, the time and space complexity of an algorithm can be expressed using mathematical formulas, which can be used to estimate its time and space requirements.
- **Asymptotic analysis:** Asymptotic analysis is a technique for analyzing the behavior of an algorithm as the input size grows to infinity. It involves estimating the upper and lower bounds on the algorithm's time and space complexity using big-O, big-Omega, and big-Theta notations.

By using these techniques, the standard analysis technique estimates an algorithm's time and space complexity and determines its suitability for a given application or problem size. It can use various methods, such as counting basic operations, using mathematical formulas to estimate time and space complexity, or asymptotic analysis, which estimates the upper and lower bounds on the algorithm's time and space complexity using big-O, big-Omega, and big-Theta notations.


## Recurrence relations

In computer science and mathematics, a recurrence relation is an equation that recursively defines a sequence or sequence of functions. Recurrence relations are often used to analyze the time complexity of algorithms or the behavior of recursive functions.

In the context of algorithm analysis, a recurrence relation is used to describe the running time of an algorithm in terms of the size of its input. Typically, a recurrence relation will express the running time of an algorithm on an input of size n in terms of the running time of the algorithm on smaller inputs. Solving the recurrence relation then involves finding a closed-form expression for the running time of the algorithm on an input of size n.

There are various techniques for solving recurrence relations, including substitution, recurrence tree, and master theorem. Solving recurrence relations can help us determine the time complexity of algorithms, which is crucial in analyzing the efficiency of algorithms and comparing different algorithms for the same problem.

There are several methods to solve recurrence relations, including:

- **Substitution method:** 

    The substitution method is one of the methods used to solve recurrence relations. In this method, we guess a solution for the recurrence relation and then prove it by induction.

    Here are the steps to solve a recurrence relation using the substitution method:

    ```
    Guess the form of the solution.
    Use mathematical induction to prove that the guess is correct.
    Let's take an example to understand how the substitution method works. Suppose we have a recurrence relation:

    T(n) = 2T(n/2) + n, where T(1) = 1

    To solve this recurrence relation using the substitution method, we will follow the steps:

    Guess the form of the solution:
    We can guess that the solution is T(n) = O(nlogn).

    Prove that the guess is correct using mathematical induction:
    Base case: T(1) = 1, which satisfies the guess.

    Inductive step: Assume that T(k) <= cklogk for all k < n, where c is some constant.

    We have:
    T(n) = 2T(n/2) + n
    <= 2cn/2 * log(n/2) + n
    = cnlogn - cnlog2 + n
    = cnlogn - cn + n
    <= cnlogn

    The last inequality holds if we choose c >= 1 and n > 1.
    ```

    Hence, by mathematical induction, we have proved that `T(n) = O(nlogn)` is a valid solution to the given recurrence relation.

    Therefore, the solution to the recurrence relation `T(n) = 2T(n/2) + n`, where `T(1) = 1, is T(n) = O(nlogn)`.


- **Iteration method:** 

    This involves iteratively plugging in the recurrence relation into itself and simplifying until a pattern emerges.

    The iteration method is another technique used to solve recurrence relations. It involves repeatedly substituting the previous value into the original equation until a pattern or closed-form solution emerges. The iteration method is particularly useful when the substitution method fails or becomes too complex.

    Here is an example of how to solve a recurrence relation using the iteration method:

    ```
    Consider the recurrence relation: T(n) = T(n-1) + 3n, where T(1) = 1.

    We want to find a closed-form solution for T(n).

    To use the iteration method, we start by computing the first few terms of the sequence:

    T(1) = 1
    T(2) = T(1) + 3(2) = 7
    T(3) = T(2) + 3(3) = 16
    T(4) = T(3) + 3(4) = 28

    We notice that the sequence appears to be a quadratic polynomial of the form T(n) = an^2 + bn + c. To find the coefficients a, b, and c, we use the initial condition T(1) = 1:

    T(1) = a(1)^2 + b(1) + c = 1
    a + b + c = 1

    We also use the recurrence relation to obtain two more equations:

    T(2) = a(2)^2 + b(2) + c = 7
    4a + 2b + c = 7

    T(3) = a(3)^2 + b(3) + c = 16
    9a + 3b + c = 16

    We can now solve these three equations simultaneously to find the values of a, b, and c. Solving the equations yields:

    a = 1
    b = -1
    c = 1

    Therefore, the closed-form solution for the recurrence relation T(n) = T(n-1) + 3n, where T(1) = 1, is T(n) = n^2 - n + 1.
    ```

- **Master theorem:** 

    This is a formula that gives a solution to recurrence relations of the form `T(n) = aT(n/b) + f(n)`, where `a ≥ 1`, `b > 1`, and `f(n)` is a given function.

    The Master theorem is a formula for solving divide-and-conquer recurrence relations. It provides a way to find a closed-form solution for a recurrence relation by identifying its complexity using three cases. The theorem is often used in the analysis of algorithms to determine their time complexity.

    ```
    The theorem states that if a recurrence relation of the form:

    T(n) = aT(n/b) + f(n)

    where a is the number of subproblems, each of size n/b, and f(n) is the time complexity of the divide-and-conquer step, then the time complexity T(n) can be expressed as:

    If f(n) = O(n^log_b a-ϵ), then T(n) = Θ(n^log_b a)

    If f(n) = Θ(n^log_b a), then T(n) = Θ(n^log_b a * log n)

    If f(n) = Ω(n^log_b a+ϵ), and if a * f(n/b) ≤ c * f(n) for some constant c < 1 and sufficiently large n, then T(n) = Θ(f(n))

    Here, ϵ is a small positive constant and log_b a represents the logarithm of a to the base b.

    Let's consider an example: T(n) = 2T(n/2) + n.

    Here, a = 2, b = 2 and f(n) = n.

    Using the Master theorem, we can determine the time complexity of this recurrence relation as follows:

    log_b a = log_2 2 = 1

    Case 2 applies because f(n) = Θ(n^log_b a).

    Therefore, T(n) = Θ(n^log_b a * log n) = Θ(n log n).
    ```

    Hence, the time complexity of the algorithm is O(n log n).

- **Generating functions:** 
    
    This involves representing the sequence of numbers generated by the recurrence relation as a power series, and then manipulating this series to obtain a closed-form solution.

    Generating functions are mathematical tools used to represent sequences of numbers as power series. They provide a powerful way to study and manipulate sequences of numbers and are widely used in combinatorics, number theory, and other areas of mathematics.

    The basic idea behind generating functions is to encode the sequence of numbers into a polynomial or power series. 

    ```
    The generating function for a sequence {an} is defined as:

    G(x) = a0 + a1x + a2x^2 + a3x^3 + ...

    For example, the generating function for the sequence {1, 1, 2, 3, 5, 8, 13, ...} (the Fibonacci sequence) is:

    G(x) = 1 + x + 2x^2 + 3x^3 + 5x^4 + 8x^5 + 13x^6 + ...

    Generating functions can be used to solve a variety of problems, including counting problems and recurrence relations. The following is an example of how generating functions can be used to solve a counting problem:

    Example: How many ways are there to roll a sum of 7 with two dice?

    Solution: We can represent the sum of two dice rolls as the coefficients of the generating function:

    G(x) = (x + x^2 + x^3 + x^4 + x^5 + x^6)^2

    Expanding this out gives:

    G(x) = x^2 + 2x^3 + 3x^4 + 4x^5 + 5x^6 + 6x^7 + 5x^8 + 4x^9 + 3x^10 + 2x^11 + x^12

    The coefficient of x^7 is 6, which means there are 6 ways to roll a sum of 7 with two dice.
    ```

    Generating functions can also be used to solve recurrence relations, as demonstrated by the master theorem. Overall, generating functions provide a powerful and versatile tool for solving a wide range of mathematical problems.

- **Recursion trees:** 
   
    This involves drawing a tree-like diagram to visualize the recursion involved in the recurrence relation, and then computing the total work done by summing the work done at each level of the tree.

    Recursion trees are a graphical representation of the recursive function calls that occur in a recursive algorithm. Each node in the tree represents the cost of a single subproblem, and the branches represent the subproblems created by that node.

    To analyze the time complexity of the recursive algorithm, we sum the costs of all nodes in the tree, which gives us the total cost of the algorithm.

    For example, consider the following recursive function that calculates the nth Fibonacci number:

    ```
    function fibonacci(n):
        if n <= 1:
            return n
        else:
            return fibonacci(n-1) + fibonacci(n-2)
    ```

    To construct the recursion tree for `fibonacci(5)`, we start with the root node representing the cost of the `fibonacci(5)` call, which is equal to the sum of the costs of its two child nodes `fibonacci(4)` and `fibonacci(3`). We then continue constructing the tree by recursively adding nodes until we reach the base case, which are the nodes with no children.

    ```
      The resulting recursion tree for fibonacci(5) looks like this:

                  fibonacci(5)
                  /          \
        fibonacci(4)       fibonacci(3)
        /        \          /        \
    fibonacci(3) fibonacci(2) fibonacci(2) fibonacci(1)
      /    \
    fibonacci(2) fibonacci(1)

    ```

    To calculate the time complexity of `fibonacci(5)`, we sum the costs of all the nodes in the tree, which is equal to 15. Therefore, the time complexity of `fibonacci(5)` is `O(15)`, which simplifies to `O(2^n)`.

    Recursion trees can be used to analyze the time complexity of many recursive algorithms and can help in understanding the behavior of the algorithm as the input size increases.


## Resources
- https://www.geeksforgeeks.org/time-complexities-of-different-data-structures/
- https://devopedia.org/algorithmic-complexity
- https://www.javatpoint.com/daa-analyzing-algorithm-control-structure
- https://www.javatpoint.com/daa-recurrence-relation
- https://www.youtube.com/watch?v=xLetJpcjHS0&list=PLBlnK6fEyqRj9lld8sWIUNwlKfdUoPd1Y
